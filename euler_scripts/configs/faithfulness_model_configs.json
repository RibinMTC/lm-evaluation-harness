{
  "base_config_dir": "configs/faithfulness_benchmark_final_configs",
  "model_checkpoints": [
    "mtc/LeoLM-leo-mistral-hessianai-7b-all-labels-german-classification-finetuned",
    "mtc/LeoLM-leo-mistral-hessianai-7b-classification-with-explanation-3-epochs-finetuned",
    "LeoLM/leo-hessianai-13b",
    "google/flan-ul2",
    "LeoLM/leo-hessianai-7b",
    "LeoLM/leo-mistral-hessianai-7b",
    "mistralai/Mistral-7B-v0.1",
    "mistralai/Mistral-7B-Instruct-v0.2",
    "NousResearch/Llama-2-7b-hf",
    "NousResearch/Llama-2-13b-hf",
    "mtc/LeoLM-leo-hessianai-13b-all-labels-classification-english-one-epoch-qlora-4bit-merged",
    "mtc/LeoLM-leo-hessianai-13b-all-labels-classification-one-epoch-qlora-4bit-merged",
    "mtc/LeoLM-leo-hessianai-13b-all-labels-german-classification-with-explanation-finetuned",
    "mtc/LeoLM-leo-mistral-hessianai-7b-classification-with-explanation-neftune-finetuned",
    "mtc/LeoLM-leo-mistral-hessianai-7b-all-labels-german-classification-with-explanation-500-finetuned",
    "mtc/LeoLM-leo-mistral-hessianai-7b-all-labels-german-classification-with-explanation-250-finetuned",
    "mtc/LeoLM-leo-mistral-hessianai-7b-all-labels-german-classification-with-explanation-100-finetuned",
    "mtc/LeoLM-leo-mistral-hessianai-7b-all-labels-german-classification-with-explanation-50-finetuned",
    "mtc/LeoLM-leo-mistral-hessianai-7b-all-labels-german-classification-with-explanation-finetuned",
    "mtc/microsoft-Orca-2-7b-classification-with-explanation-english-prompt-finetuned",
    "mtc/LeoLM-leo-mistral-hessianai-7b-xnli-german-classification-context-512-finetuned-v2",
    "mtc/LeoLM-leo-mistral-hessianai-7b-xnli-absinth-finetuned",
    "ehartford/dolphin-2.2.1-mistral-7b",
    "microsoft/Orca-2-7b",
    "mtc/ehartford-dolphin-2.2.1-mistral-7b-classification-finetuned",
    "mtc/ehartford-dolphin-2.2.1-mistral-7b-classification-with-explanation-finetuned",
    "mtc/mistralai-Mistral-7B-v0.1-classification-with-explanation-3-epochs-finetuned",
    "mtc/leo-mistral-absinth-finetuned-3-epochs-GGUF",
    "mtc/LeoLM-leo-mistral-hessianai-7b-paraphrase-absinth-with-explanation-finetuned",
    "mtc/LeoLM-leo-mistral-hessianai-7b-backtranslation-absinth-with-explanation-finetuned",
    "mtc/LeoLM-leo-mistral-hessianai-7b-classification-with-mixtral-explanation-3-epochs-finetuned",
    "mtc/LeoLM-leo-mistral-7b-mixtral-and-gpt4-explanation-3-epochs-finetuned",
    "mtc/LeoLM-leo-mistral-7b-mixtral-and-gpt4-explanation-5-epochs-finetuned",
    "mtc/LeoLM-leo-mistral-7b-mixtral-and-gpt4-explanation-3-epochs-loftq-finetuned",
    "mtc/upstage-SOLAR-10.7B-v1.0-classification-with-mixtral-explanation-3-epochs-finetuned",
    "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "mtc/LeoLM-leo-mistral-7b-mixtral-and-gpt4-explanation-no-packing-1-epoch-finetuned",
    "mtc/LeoLM-leo-mistral-hessianai-7b-no-packing-finetuned",
    "mtc/mistralai-Mistral-7B-v0.1-7b-xsum-with-all-explanation-3-epochs-full-dataset-eot-finetuned",
    "mtc/mistralai-Mistral-7B-v0.1-7b-xsum-with-all-explanation-3-epochs-full-dataset-eot-2-finetuned",
    "/cluster/scratch/cribin/llm_finetuning/custom_xsum_faithfulness_multi_label_classification_with_explanation",
    "mtc/LeoLM-leo-mistral-hessianai-7b-xnli-with-explanation-1000-3-epoch-finetuned"
  ],
  "few_shot_values": {"lower_range":  0,
    "upper_range": 32
  },
  "max_context_length_values" : [1024, 2048, 4096, 8192],
  "tasks_with_prompt_templates": {
    "faithfulness_benchmark_final_swisstext23_benchmark": "configs/prompt_templates/faithfulness_benchmark_final_swisstext23_benchmark.json",
    "faithfulness_benchmark_final_swisstext23_multi_label": "configs/prompt_templates/faithfulness_benchmark_final_swisstext23_multi_label.json",
    "faithfulness_benchmark_final_swisstext23_with_explanation_multi_label": "configs/prompt_templates/faithfulness_benchmark_final_swisstext23_with_explanation_multi_label.json",
    "full_disagreements_faithfulness_benchmark_final_swisstext23_with_explanation_multi_label": "configs/prompt_templates/faithfulness_benchmark_final_swisstext23_with_explanation_multi_label.json",
    "absinth_with_explanations_empty_article": "configs/prompt_templates/faithfulness_benchmark_final_swisstext23_with_explanation_multi_label.json",
    "absinth_with_explanations_unrelated_article": "configs/prompt_templates/faithfulness_benchmark_final_swisstext23_with_explanation_multi_label.json",
    "absinth_with_explanations_article_with_noise": "configs/prompt_templates/faithfulness_benchmark_final_swisstext23_with_explanation_multi_label.json",
    "absinth_with_explanations_faithful_double_negation": "configs/prompt_templates/faithfulness_benchmark_final_swisstext23_with_explanation_multi_label.json",
    "absinth_self_consistency": "configs/prompt_templates/faithfulness_benchmark_final_swisstext23_with_explanation_multi_label.json",
    "xnli_multi_label": "configs/prompt_templates/faithfulness_benchmark_final_swisstext23_multi_label.json",
    "xnli_with_explanation_multi_label": "configs/prompt_templates/faithfulness_benchmark_final_swisstext23_with_explanation_multi_label.json",
    "xsum_faith_multi_label": "configs/prompt_templates/faithfulness_benchmark_final_swisstext23_multi_label.json",
    "xsum_faith_with_explanation_multi_label": "configs/prompt_templates/faithfulness_benchmark_final_swisstext23_with_explanation_multi_label.json",
    "seahorse_attribution_with_explanation_multi_label": "configs/prompt_templates/faithfulness_benchmark_final_swisstext23_with_explanation_multi_label.json",
    "absinth_reasoning_generation" : "configs/prompt_templates/absinth_reasoning_generation.json",
    "xnli_reasoning_generation" : "configs/prompt_templates/absinth_reasoning_generation.json",
    "xsum_faith_reasoning_generation" : "configs/prompt_templates/absinth_reasoning_generation.json"
  },
  "seeds": [5, 10, 42, 100],
  "few_shot_sampling_strategies" : ["stratified", "packed"]
}