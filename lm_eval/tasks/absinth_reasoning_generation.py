import evaluate

from lm_eval.base import Task, rf
from lm_eval.metrics import mean

wordlength = evaluate.load("word_length", module_type="measurement")


class AbsinthReasoningGenerationTask(Task):
    def doc_to_target(self, doc):
        pass

    def process_results(self, doc, results):
        results = wordlength.compute(data=results[0])
        return results

    def aggregation(self):
        return {
            "average_word_length": mean
        }

    def higher_is_better(self):
        return {
            "average_word_length": False
        }

    VERSION = 0
    # dataset as denoted in HuggingFace `datasets`.
    DATASET_PATH = "mtc/final_german_faithfulness_benchmark"
    # `DATASET_PATH`. If there aren't specific subsets you need, leave this as `None`.
    DATASET_NAME = None
    article_key_name = "lead_with_article"
    summary_key_name = "text"
    label_key_name = "label"

    default_prompt_template = """Reply ONLY in German. For the specified article, sentence, and label, explain in detail why the label was selected and the others label were not chosen. The label 'Faithful' indicates the sentence aligns with the article's content, 'Intrinsic Hallucination' means it contradicts or misrepresents the article by swapping entities, dates ,location etc. and 'Extrinsic hallucination' suggests it includes information not present in the article. Don't reply with: the label was chosen... but instead simply provide a concise explanation without mentioning the label. If the label is Intrinsic Hallucination, don't reply with: The information is not contained in the article. Think step by step before providing final explanation.
Artikel: {article}
Satz: {sentence}
Label: {label}
Erkl√§rung:"""

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def test_docs(self):
        if self.has_test_docs():
            return self.dataset["train"]

    def doc_to_text(self, doc):
        if self.prompt_template is None:
            self.prompt_template = self.default_prompt_template

        prompt = self.prompt_template.format(article=doc[self.article_key_name],
                                             sentence=doc[self.summary_key_name], label=doc[self.label_key_name])

        return prompt

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or
            test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        continuation = rf.greedy_until(ctx, {"until": ["\n"]})
        return continuation
