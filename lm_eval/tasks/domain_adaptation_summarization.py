"""
The Task is based on the arxiv, govreport and pubmed Datasets for summarization
"""

import evaluate
import nltk

from lm_eval.base import Task, rf
from lm_eval.fragments import Fragments
from lm_eval.metrics import mean

summarization_metric = evaluate.load("rouge")


def _rouge_metric(predictions, references, rouge_type=None):
    result = summarization_metric.compute(
        predictions=[predictions],
        references=[references],
        use_stemmer=True,
    )[rouge_type]
    return result


def _rouge_agg(key, items):
    predictions, references = zip(*items)
    result = _rouge_metric(
        predictions=predictions, references=references, rouge_type=key
    )
    return result


class DomainAdaptationSummarizationBaseTask(Task):
    VERSION = 0
    # dataset as denoted in HuggingFace `datasets`.
    DATASET_PATH = None
    # `DATASET_PATH`. If there aren't specific subsets you need, leave this as `None`.
    DATASET_NAME = None
    LANGUAGE = "english"

    article_key = ""
    summary_key = ""

    system_message = "You are an expert at summarization. "
    prompt_suffix = "\nSUMMARY:"
    zero_shot_prompt_template = "Proceed to summarize the following text. TEXT: {article}"
    few_shot_prompt_template = zero_shot_prompt_template + prompt_suffix + "{summary}\n"

    max_generation_length = None

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        if self.has_training_docs():
            if self._training_docs is None:
                self._training_docs = self.dataset["train"]
            return self._training_docs

    def validation_docs(self):
        if self.has_validation_docs():
            return self.dataset["validation"]

    @staticmethod
    def count_words(text):
        words = text.split()  # Split the text into words
        return len(words)

    def test_docs(self):
        if self.has_test_docs():
            return self.dataset["test"]

    def doc_to_text(self, doc):
        if "{summary}" in self.prompt_template:
            prompt = self.prompt_template.format(article=doc[self.article_key], summary=doc[self.summary_key])
        else:
            prompt = self.prompt_template.format(article=doc[self.article_key])

        return prompt

    def doc_to_target(self, doc):
        summary = doc[self.summary_key]
        return summary

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or
            test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        continuation = rf.greedy_until(ctx, {"max_length": self.max_generation_length,
                                             "request_suffix": self.prompt_suffix})
        return continuation

    def get_fewshot_prompt(self, doc, num_fewshot, rnd):
        # Raise an error if no training documents are available
        if not self.has_training_docs():
            raise ValueError("Training documents are required for few-shot prompting!")

        training_docs = self.training_docs().to_list()  # Load training documents

        selected_few_shot_samples = rnd.sample(training_docs, num_fewshot)

        self.prompt_template = self.few_shot_prompt_template
        formatted_few_shot_examples = [
            self.doc_to_text(selected_few_shot_sample) for selected_few_shot_sample in selected_few_shot_samples
        ]
        formatted_few_shot_examples = ''.join(formatted_few_shot_examples)

        self.prompt_template = self.zero_shot_prompt_template
        zero_shot_example = self.doc_to_text(doc)

        full_few_shot_prompt = self.system_message + formatted_few_shot_examples + zero_shot_example

        return full_few_shot_prompt

    def fewshot_context(self, doc, num_fewshot, provide_description=None, rnd=None,
                        description=None, fewshot_sampling: str = None):
        # Ensure a random generator is provided
        if rnd is None:
            raise ValueError("A `random.Random` generator argument must be provided to `rnd`")

        # Handle case with no few-shot examples
        if num_fewshot == 0:
            self.prompt_template = self.zero_shot_prompt_template
            full_prompt = self.system_message + self.doc_to_text(doc)
        else:
            full_prompt = self.get_fewshot_prompt(doc, num_fewshot, rnd)

        return full_prompt

    def postprocess_text(self, prediction, reference):
        prediction = prediction.strip()
        reference = reference.strip()

        # rougeLSum expects newline after each sentence
        prediction = "\n".join(nltk.sent_tokenize(prediction, language=self.LANGUAGE))
        reference = "\n".join(nltk.sent_tokenize(reference, language=self.LANGUAGE))
        return prediction, reference

    def round_to_3_decimals(self, value: float) -> float:
        return round(value, 3)

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        assert len(results) == 1

        prediction, reference = self.postprocess_text(results[0], doc[self.summary_key])

        return {
            "rouge1": self.round_to_3_decimals(_rouge_metric(prediction, reference, "rouge1")),
            "rouge2": self.round_to_3_decimals(_rouge_metric(prediction, reference, "rouge2")),
            "rougeL": self.round_to_3_decimals(_rouge_metric(prediction, reference, "rougeL"))
        }

    def aggregation(self):
        """
        :returns: {str: [metric_score] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metric scores
        """
        return {
            "rouge1": mean,
            "rouge2": mean,
            "rougeL": mean
        }

    def higher_is_better(self):
        """
                :returns: {str: bool}
                    A dictionary where keys are the names of submetrics and values are
                    whether a higher value of the submetric is better
                """
        return {"rouge1": True, "rouge2": True, "rougeL": True}


class ArxivDomainAdaptationSummarizationTask(DomainAdaptationSummarizationBaseTask):
    DATASET_PATH = "ccdv/arxiv-summarization"
    DATASET_NAME = "document"

    article_key = "article"
    summary_key = "abstract"
    max_generation_length = 256


class Arxiv2ShotDomainAdaptationSummarizationTask(ArxivDomainAdaptationSummarizationTask):
    DATASET_PATH = "anumafzal94/arxiv_2-shot"
    DATASET_NAME = None
    article_key = "text"
    summary_key = "summary"


class PubmedDomainAdaptationSummarizationTask(DomainAdaptationSummarizationBaseTask):
    DATASET_PATH = "ccdv/pubmed-summarization"
    DATASET_NAME = "document"

    article_key = "article"
    summary_key = "abstract"
    max_generation_length = 256


class Pubmed2ShotDomainAdaptationSummarizationTask(PubmedDomainAdaptationSummarizationTask):
    DATASET_PATH = "anumafzal94/pubmed-2shot-4096"
    DATASET_NAME = None
    article_key = "text"
    summary_key = "summary"


class GovReportDomainAdaptationSummarizationTask(DomainAdaptationSummarizationBaseTask):
    DATASET_PATH = "ccdv/govreport-summarization"

    article_key = "report"
    summary_key = "summary"
    max_generation_length = 1024
