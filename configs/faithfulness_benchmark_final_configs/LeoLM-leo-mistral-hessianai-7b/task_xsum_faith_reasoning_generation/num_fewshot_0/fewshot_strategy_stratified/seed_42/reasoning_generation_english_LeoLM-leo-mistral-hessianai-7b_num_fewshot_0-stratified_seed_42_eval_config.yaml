batch_size: '1'
check_integrity: false
data_sampling: null
decontamination_ngrams_path: null
description_dict_path: null
device: null
end_range: 10
fewshot_sampling: stratified
model: hf-causal-experimental
model_args: pretrained=LeoLM/leo-mistral-hessianai-7b,trust_remote_code=False,use_accelerate=True,dtype=bfloat16,load_in_8bit=False,attn_implementation=flash_attention_2,max_length=4096
no_cache: true
num_fewshot: 0
output_base_path: results
output_path: null
seed: 42
start_range: null
task_configs:
-   prompt_template: configs/prompt_templates/absinth_reasoning_generation.json
    prompt_version: reasoning_generation_english
    task_name: xsum_faith_reasoning_generation
wandb_on: false
write_out: false
